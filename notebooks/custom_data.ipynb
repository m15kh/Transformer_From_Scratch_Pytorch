{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "db9c9cf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import TensorDataset\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "59929b89",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([464965814]), torch.Size([4669118]))"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load tokens from pytorch file\n",
    "tokenized_train_samples = torch.load('/home/fteam6/m15kh/Transformer_From_Scratch_Pytorch/checkpoints/tokenized_train_samples_vocab_10k.pt')\n",
    "tokenized_valid_samples = torch.load('/home/fteam6/m15kh/Transformer_From_Scratch_Pytorch/checkpoints/tokenized_valid_samples_vocab_10k.pt')\n",
    "tokenized_train_samples.shape, tokenized_valid_samples.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "8543de26",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_data(tokens, seq_len):\n",
    "    \"\"\"\n",
    "    Converts a flat list of token IDs into a 2D tensor where each row has length `seq_len`.\n",
    "\n",
    "    Args:\n",
    "        token_list (list or array-like): Flat list of token IDs.\n",
    "        seq_len (int): Desired sequence length per row.\n",
    "\n",
    "    Returns:\n",
    "        torch.Tensor: 2D tensor of shape (num_sequences, seq_len).\n",
    "    \"\"\"\n",
    "\n",
    "    # Trim tokens so that total length is divisible by seq_len\n",
    "    n_tokens = (tokens.shape[0] // seq_len) * seq_len\n",
    "    tokens = tokens[:n_tokens]\n",
    "\n",
    "    # Reshape to 2D tensor\n",
    "    return tokens.view(-1, seq_len)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "1b9a8a75",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3632545, 128])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seq_len = 128  # Desired sequence length for each row\n",
    "tokenized_train_samples = prepare_data(tokenized_train_samples, seq_len)\n",
    "tokenized_train_samples.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "cbeb98c8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([   1,  316,  252,   13,  155,  293,  342,  396,  260,  491,  155, 3628,\n",
       "          212,  205,  654,   15,  209,  599,  200,  178, 2858,  162,  255,  238,\n",
       "          200,  683,  200,  178, 2012,   15,  260,  340,  162,  844,  159, 3628,\n",
       "          238,  205,  261,   13,  246,  234,  356, 5090,  155, 2120,  241,  205,\n",
       "         2392,   15,  132,  132,  239,  364,  162,  205,  261,  161,  223,   13,\n",
       "          225,  666,   13,  231,  491,  636, 3628,   15, 1176,  242,  844,  200,\n",
       "          238,  414,  161, 5090,  547, 2392,  373,  761,  261,  394,  161,  223,\n",
       "           13,  225,  727,   13,  260,   13,  259,  367,  844,  159, 3628,  161,\n",
       "         1200,  522, 2392,  311,  132,  132, 4517,   13,  258, 1550,  159, 3628,\n",
       "          161, 7822,  159, 2120,  241,  260,  267, 2392,   15,  305,  178,  281,\n",
       "         2858,  262,  344,  683,  258,  325, 2394]),\n",
       " tensor([ 316,  252,   13,  155,  293,  342,  396,  260,  491,  155, 3628,  212,\n",
       "          205,  654,   15,  209,  599,  200,  178, 2858,  162,  255,  238,  200,\n",
       "          683,  200,  178, 2012,   15,  260,  340,  162,  844,  159, 3628,  238,\n",
       "          205,  261,   13,  246,  234,  356, 5090,  155, 2120,  241,  205, 2392,\n",
       "           15,  132,  132,  239,  364,  162,  205,  261,  161,  223,   13,  225,\n",
       "          666,   13,  231,  491,  636, 3628,   15, 1176,  242,  844,  200,  238,\n",
       "          414,  161, 5090,  547, 2392,  373,  761,  261,  394,  161,  223,   13,\n",
       "          225,  727,   13,  260,   13,  259,  367,  844,  159, 3628,  161, 1200,\n",
       "          522, 2392,  311,  132,  132, 4517,   13,  258, 1550,  159, 3628,  161,\n",
       "         7822,  159, 2120,  241,  260,  267, 2392,   15,  305,  178,  281, 2858,\n",
       "          262,  344,  683,  258,  325, 2394,  161]))"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_set = TensorDataset(tokenized_train_samples[:, :-1], tokenized_train_samples[:, 1:])\n",
    "train_set[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "88f09b34",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_iter = iter(train_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "1e927adf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.68 μs ± 19.4 ns per loop (mean ± std. dev. of 7 runs, 100,000 loops each)\n"
     ]
    }
   ],
   "source": [
    "%timeit next(train_iter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "96206ff5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "sys.path.append(os.path.abspath(os.path.join(os.getcwd(), '../')))\n",
    "from Transformer_From_Scratch_Pytorch.data.data_loader import TinyStoriesDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "142bfaad",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = TinyStoriesDataset(tokenized_train_samples, 128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "9764c728",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'torch.Size' object cannot be interpreted as an integer",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[30]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtrain_dataset\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mTypeError\u001b[39m: 'torch.Size' object cannot be interpreted as an integer"
     ]
    }
   ],
   "source": [
    "len(train_dataset)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
